{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import glob\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def read_data(file_pattern):\n",
    "    # Use glob to get all file paths matching the pattern\n",
    "    file_paths = glob.glob(file_pattern)\n",
    "\n",
    "    # Read and concatenate all CSV files into one DataFrame\n",
    "    data_frames = [pd.read_csv(file) for file in file_paths]\n",
    "    data = pd.concat(data_frames, ignore_index=True)\n",
    "    data = data[data['Type'] == 'multifocal']\n",
    "    return data\n",
    "\n",
    "\n",
    "def clean_data(data):\n",
    "    X = data[['OD Sphere', 'OD Cylinder', 'OD Axis', 'OD Add',\n",
    "            'OS Sphere', 'OS Cylinder', 'OS Axis', 'OS Add']]\n",
    "\n",
    "    # Handle missing values (if any)\n",
    "    X.fillna(0, inplace=True)\n",
    "\n",
    "    # Normalize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    # Calculate Z-scores\n",
    "    z_scores = np.abs((X_scaled - X_scaled.mean(axis=0)) / X_scaled.std(axis=0))\n",
    "\n",
    "    # Filter out rows where any Z-score is above the threshold (e.g., 3)\n",
    "    threshold = 3\n",
    "    return data[(z_scores < threshold).all(axis=1)]\n",
    "\n",
    "data = read_data('dispense_report*.csv')\n",
    "data = data[data['dispense type'] == 'DISPENSED']\n",
    "data = clean_data(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "X = data[['OD Sphere', 'OD Cylinder', 'OD Axis', 'OD Add',\n",
    "        'OS Sphere', 'OS Cylinder', 'OS Axis', 'OS Add']]\n",
    "# Standardize the filtered data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "# Perform hierarchical clustering on filtered data\n",
    "Z = linkage(X_scaled, method='ward')\n",
    "\n",
    "\n",
    "\n",
    "# Plot the dendrogram\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(Z)\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Glasses')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cut the dendrogram to form clusters (e.g., 5 clusters)\n",
    "num_clusters = 8\n",
    "data['cluster'] = fcluster(Z, t=num_clusters, criterion='maxclust')\n",
    "\n",
    "# Add cluster labels to the original data\n",
    "data['cluster'] = data['cluster'].astype(str)  # Convert to string for easier analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Explained variance ratio\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
    "\n",
    "# Loading scores\n",
    "loading_scores = pd.DataFrame(pca.components_.T, columns=['PC1', 'PC2'], index=X.columns)\n",
    "print(\"Loading scores:\\n\", loading_scores)\n",
    "\n",
    "# Plot the PCA components\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=data['cluster'].astype(int), cmap='viridis', alpha=0.5)\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.title('PCA of Dispense Report Data')\n",
    "plt.legend(*scatter.legend_elements(), title=\"Clusters\")\n",
    "plt.show()\n",
    "\n",
    "# Plot loading scores\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "loading_scores['PC1'].plot(kind='bar', ax=ax[0])\n",
    "ax[0].set_title('Loading Scores for PC1')\n",
    "ax[0].set_ylabel('Loading Score')\n",
    "ax[0].set_xlabel('Feature')\n",
    "ax[0].set_ylim(-0.5, 0.5)\n",
    "\n",
    "loading_scores['PC2'].plot(kind='bar', ax=ax[1])\n",
    "ax[1].set_title('Loading Scores for PC2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Explained variance ratio\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
    "\n",
    "# Loading scores\n",
    "loading_scores = pd.DataFrame(pca.components_.T, columns=['PC1', 'PC2', 'PC3'], index=X.columns)\n",
    "print(\"Loading scores:\\n\", loading_scores)\n",
    "\n",
    "# Plot the PCA components in 3D\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=data['cluster'].astype(int), cmap='viridis', alpha=0.5)\n",
    "\n",
    "ax.set_xlabel('PCA Component 1')\n",
    "ax.set_ylabel('PCA Component 2')\n",
    "ax.set_zlabel('PCA Component 3')\n",
    "ax.set_title('3D PCA of Dispense Report Data')\n",
    "legend1 = ax.legend(*scatter.legend_elements(), title=\"Clusters\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "# Plot loading scores\n",
    "fig, ax = plt.subplots(1, 3, figsize=(18, 6))\n",
    "loading_scores['PC1'].plot(kind='bar', ax=ax[0])\n",
    "ax[0].set_title('Loading Scores for PC1')\n",
    "ax[0].set_ylabel('Loading Score')\n",
    "ax[0].set_xlabel('Feature')\n",
    "ax[0].set_ylim(-0.5, 0.5)\n",
    "\n",
    "loading_scores['PC2'].plot(kind='bar', ax=ax[1])\n",
    "ax[1].set_title('Loading Scores for PC2')\n",
    "ax[1].set_ylabel('Loading Score')\n",
    "ax[1].set_xlabel('Feature')\n",
    "ax[1].set_ylim(-0.5, 0.5)\n",
    "\n",
    "loading_scores['PC3'].plot(kind='bar', ax=ax[2])\n",
    "ax[2].set_title('Loading Scores for PC3')\n",
    "ax[2].set_ylabel('Loading Score')\n",
    "ax[2].set_xlabel('Feature')\n",
    "ax[2].set_ylim(-0.5, 0.5)\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess inventory data similarly\n",
    "inventory_data = read_data('inventory*.csv')\n",
    "inventory_data = clean_data(inventory_data)\n",
    "\n",
    "X_inventory = inventory_data[['OD Sphere', 'OD Cylinder', 'OD Axis', 'OD Add',\n",
    "                              'OS Sphere', 'OS Cylinder', 'OS Axis', 'OS Add']].fillna(0)\n",
    "X_inventory_scaled = scaler.transform(X_inventory)\n",
    "\n",
    "# Compute cluster centroids from dispensed data\n",
    "centroids = []\n",
    "for cluster_id in sorted(data['cluster'].unique(), key=int):\n",
    "    cluster_points = X_scaled[data['cluster'] == cluster_id]\n",
    "    centroids.append(cluster_points.mean(axis=0))\n",
    "centroids = np.array(centroids)\n",
    "\n",
    "# Assign each inventory item to the nearest centroid\n",
    "distances = np.sqrt(((X_inventory_scaled[:, None] - centroids) ** 2).sum(axis=2))\n",
    "nearest_cluster_indices = distances.argmin(axis=1)\n",
    "inventory_data['cluster'] = (nearest_cluster_indices + 1).astype(str)\n",
    "\n",
    "# Compute absolute and relative frequency in the dispensed data\n",
    "dispense_cluster_count = data['cluster'].value_counts().rename('dispense_cluster_count')\n",
    "dispense_cluster_freq = data['cluster'].value_counts(normalize=True).rename('dispense_cluster_frequency')\n",
    "\n",
    "# Compute absolute and relative frequency in the inventory data\n",
    "inventory_cluster_count = inventory_data['cluster'].value_counts().rename('inventory_cluster_count')\n",
    "inventory_cluster_freq = inventory_data['cluster'].value_counts(normalize=True).rename('inventory_cluster_frequency')\n",
    "\n",
    "# Create a comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'dispense_cluster_count': dispense_cluster_count,\n",
    "    'dispense_cluster_frequency': dispense_cluster_freq,\n",
    "    'inventory_cluster_count': inventory_cluster_count,\n",
    "    'inventory_cluster_frequency': inventory_cluster_freq\n",
    "}).fillna(0)\n",
    "\n",
    "print(comparison_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert cluster frequencies to percentages in the comparison_df\n",
    "comparison_df['dispense_cluster_percent'] = comparison_df['dispense_cluster_frequency'] * 100\n",
    "comparison_df['inventory_cluster_percent'] = comparison_df['inventory_cluster_frequency'] * 100\n",
    "\n",
    "# Plot cluster frequencies as percentages\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# We'll plot a grouped bar chart\n",
    "bar_width = 0.4\n",
    "clusters = comparison_df.index\n",
    "x_positions = range(len(clusters))\n",
    "\n",
    "ax.bar(\n",
    "    [x - bar_width/2 for x in x_positions], \n",
    "    comparison_df['dispense_cluster_percent'], \n",
    "    width=bar_width, \n",
    "    label='Dispensed (%)'\n",
    ")\n",
    "\n",
    "ax.bar(\n",
    "    [x + bar_width/2 for x in x_positions], \n",
    "    comparison_df['inventory_cluster_percent'], \n",
    "    width=bar_width, \n",
    "    label='Inventory (%)'\n",
    ")\n",
    "\n",
    "ax.set_xticks(x_positions)\n",
    "ax.set_xticklabels(clusters)\n",
    "ax.set_xlabel('Cluster')\n",
    "ax.set_ylabel('Frequency (%)')\n",
    "ax.set_title('Comparison of Cluster Frequencies: Dispensed vs. Inventory')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Prepare the data\n",
    "X = inventory_data[['OD Sphere', 'OD Cylinder', 'OD Axis', 'OD Add',\n",
    "          'OS Sphere', 'OS Cylinder', 'OS Axis', 'OS Add']]\n",
    "y = inventory_data['cluster']\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Get unique clusters\n",
    "clusters = y.unique()\n",
    "\n",
    "# Plot feature importances for each cluster\n",
    "fig, axes = plt.subplots(len(clusters), 1, figsize=(10, len(clusters) * 4))\n",
    "\n",
    "for i, cluster in enumerate(clusters):\n",
    "    # Create binary labels for the current cluster\n",
    "    y_binary = (y == cluster).astype(int)\n",
    "    \n",
    "    # Train a Random Forest classifier\n",
    "    rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "    rf.fit(X_scaled, y_binary)\n",
    "    \n",
    "    # Get feature importances\n",
    "    feature_importances = rf.feature_importances_\n",
    "    features = X.columns\n",
    "    \n",
    "    # Plot feature importances\n",
    "    axes[i].barh(features, feature_importances)\n",
    "    axes[i].set_xlabel('Feature Importance')\n",
    "    axes[i].set_ylabel('Feature')\n",
    "    axes[i].set_title(f'Feature Importances for Cluster {cluster}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
