{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import glob\n",
    "%matplotlib inline\n",
    "\n",
    "# Define the file pattern to match all CSV files\n",
    "file_pattern = 'dispense_report_*.csv'\n",
    "\n",
    "# Use glob to get all file paths matching the pattern\n",
    "file_paths = glob.glob(file_pattern)\n",
    "\n",
    "# Read and concatenate all CSV files into one DataFrame\n",
    "data_frames = [pd.read_csv(file) for file in file_paths]\n",
    "data = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "# Extract relevant features for clustering\n",
    "X = data[['OD Sphere', 'OD Cylinder', 'OD Axis', 'OD Add',\n",
    "          'OS Sphere', 'OS Cylinder', 'OS Axis', 'OS Add']]\n",
    "\n",
    "# Handle missing values (if any)\n",
    "X.fillna(0, inplace=True)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Calculate Z-scores\n",
    "z_scores = np.abs((X_scaled - X_scaled.mean(axis=0)) / X_scaled.std(axis=0))\n",
    "\n",
    "# Filter out rows where any Z-score is above the threshold (e.g., 3)\n",
    "threshold = 3\n",
    "data = data[(z_scores < threshold).all(axis=1)]\n",
    "\n",
    "X = data[['OD Sphere', 'OD Cylinder', 'OD Axis', 'OD Add',\n",
    "          'OS Sphere', 'OS Cylinder', 'OS Axis', 'OS Add']]\n",
    "# Standardize the filtered data\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Perform hierarchical clustering on filtered data\n",
    "Z = linkage(X_scaled, method='ward')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the dendrogram\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(Z)\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Glasses')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n",
    "\n",
    "# Cut the dendrogram to form clusters (e.g., 5 clusters)\n",
    "num_clusters = 6\n",
    "data['cluster'] = fcluster(Z, t=num_clusters, criterion='maxclust')\n",
    "\n",
    "# Add cluster labels to the original data\n",
    "data['cluster'] = data['cluster'].astype(str)  # Convert to string for easier analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Explained variance ratio\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
    "\n",
    "# Loading scores\n",
    "loading_scores = pd.DataFrame(pca.components_.T, columns=['PC1', 'PC2'], index=X.columns)\n",
    "print(\"Loading scores:\\n\", loading_scores)\n",
    "\n",
    "# Plot the PCA components\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.5)\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.title('PCA of Dispense Report Data')\n",
    "plt.show()\n",
    "\n",
    "# Plot loading scores\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "loading_scores['PC1'].plot(kind='bar', ax=ax[0])\n",
    "ax[0].set_title('Loading Scores for PC1')\n",
    "ax[0].set_ylabel('Loading Score')\n",
    "ax[0].set_xlabel('Feature')\n",
    "ax[0].set_ylim(-0.5, 0.5)\n",
    "\n",
    "loading_scores['PC2'].plot(kind='bar', ax=ax[1])\n",
    "ax[1].set_title('Loading Scores for PC2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Explained variance ratio\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
    "\n",
    "# Loading scores\n",
    "loading_scores = pd.DataFrame(pca.components_.T, columns=['PC1', 'PC2', 'PC3'], index=X.columns)\n",
    "print(\"Loading scores:\\n\", loading_scores)\n",
    "\n",
    "# Plot the PCA components in 3D\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=data['cluster'].astype(int), cmap='viridis', alpha=0.5)\n",
    "\n",
    "ax.set_xlabel('PCA Component 1')\n",
    "ax.set_ylabel('PCA Component 2')\n",
    "ax.set_zlabel('PCA Component 3')\n",
    "ax.set_title('3D PCA of Dispense Report Data')\n",
    "legend1 = ax.legend(*scatter.legend_elements(), title=\"Clusters\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Plot loading scores\n",
    "fig, ax = plt.subplots(1, 3, figsize=(18, 6))\n",
    "loading_scores['PC1'].plot(kind='bar', ax=ax[0])\n",
    "ax[0].set_title('Loading Scores for PC1')\n",
    "ax[0].set_ylabel('Loading Score')\n",
    "ax[0].set_xlabel('Feature')\n",
    "ax[0].set_ylim(-0.5, 0.5)\n",
    "\n",
    "loading_scores['PC2'].plot(kind='bar', ax=ax[1])\n",
    "ax[1].set_title('Loading Scores for PC2')\n",
    "ax[1].set_ylabel('Loading Score')\n",
    "ax[1].set_xlabel('Feature')\n",
    "ax[1].set_ylim(-0.5, 0.5)\n",
    "\n",
    "loading_scores['PC3'].plot(kind='bar', ax=ax[2])\n",
    "ax[2].set_title('Loading Scores for PC3')\n",
    "ax[2].set_ylabel('Loading Score')\n",
    "ax[2].set_xlabel('Feature')\n",
    "ax[2].set_ylim(-0.5, 0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load inventory data\n",
    "inventory_data = pd.read_csv('inventory_sa.csv')\n",
    "\n",
    "# Extract relevant features for clustering from inventory data\n",
    "inventory_features = inventory_data[['OD Sphere', 'OD Cylinder', 'OD Axis', 'OD Add',\n",
    "                                     'OS Sphere', 'OS Cylinder', 'OS Axis', 'OS Add']]\n",
    "\n",
    "# Handle missing values (if any)\n",
    "inventory_features.fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate dispense frequency for each type of glasses from dispense data\n",
    "dispense_frequency = data.groupby(['OD Sphere', 'OD Cylinder', 'OD Axis', 'OD Add',\n",
    "                                   'OS Sphere', 'OS Cylinder', 'OS Axis', 'OS Add']).size().reset_index(name='dispense_frequency')\n",
    "\n",
    "# Merge dispense frequency with inventory data\n",
    "merged_data = pd.merge(dispense_frequency, inventory_data, on=['OD Sphere', 'OD Cylinder', 'OD Axis', 'OD Add',\n",
    "                                                               'OS Sphere', 'OS Cylinder', 'OS Axis', 'OS Add'], how='right')\n",
    "\n",
    "# Fill NaN values in dispense_frequency with 0 (for glasses that were never dispensed)\n",
    "merged_data['dispense_frequency'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze dispense and inventory by cluster\n",
    "cluster_summary = merged_data.groupby('cluster').agg({\n",
    "    'dispense_frequency': 'sum',  # Total dispenses in the cluster\n",
    "    'SKU': 'count'               # Total inventory in the cluster (count of SKUs)\n",
    "}).reset_index()\n",
    "\n",
    "print(\"Cluster Summary:\")\n",
    "print(cluster_summary)\n",
    "\n",
    "# Identify clusters with high dispense frequency and low inventory\n",
    "high_demand_clusters = cluster_summary[(cluster_summary['dispense_frequency'] > cluster_summary['dispense_frequency'].quantile(0.75)) & \n",
    "                                       (cluster_summary['SKU'] < cluster_summary['SKU'].quantile(0.25))]\n",
    "\n",
    "print(\"High-demand, low-inventory clusters:\")\n",
    "print(high_demand_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter glasses in high-demand, low-inventory clusters\n",
    "high_demand_glasses = merged_data[merged_data['cluster'].isin(high_demand_clusters['cluster'])]\n",
    "\n",
    "# Sort by dispense frequency to prioritize restocking\n",
    "high_demand_glasses = high_demand_glasses.sort_values(by='dispense_frequency', ascending=False)\n",
    "\n",
    "print(\"Glasses to restock:\")\n",
    "print(high_demand_glasses[['OD Sphere', 'OD Cylinder', 'OD Axis', 'OD Add',\n",
    "                           'OS Sphere', 'OS Cylinder', 'OS Axis', 'OS Add',\n",
    "                           'dispense_frequency', 'SKU', 'cluster']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
